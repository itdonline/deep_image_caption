{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing deep image caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3691515191822865850\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import iglob\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manager import DataManager\n",
    "from models import build_caption_model, build_image_encoder\n",
    "from caption_generator import CaptionGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_CAPTION_LENGTH = 30\n",
    "BEAM_SIZE = 1\n",
    "N_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm_train = DataManager('./data/flickr8k_train.pkl')\n",
    "\n",
    "dm_val = DataManager('./data/flickr8k_val.pkl', caption_length=dm_train.caption_length, \n",
    "                     return_image_features=False)\n",
    "val_generator = dm_val.flow(shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = list(iglob('./experiments/**/*.hdf5', recursive=True))\n",
    "model_paths = sorted(model_paths, key=lambda x: os.path.getctime(x))\n",
    "\n",
    "# model_path = model_paths[-1]  # getting most fresh weights\n",
    "model_path = './experiments/2017-08-25-17:03:54/checkpoints/checkpoint-0121.hdf5'\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class CaptionGenerator(object):\n",
    "    def __init__(self, model_path, image_encoder, vocabulary, caption_length):\n",
    "        self.model_path = model_path\n",
    "        self.caption_model = load_model(model_path)\n",
    "\n",
    "        self.image_encoder = image_encoder\n",
    "        \n",
    "        self.vocabulary = vocabulary\n",
    "        self.inversed_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
    "        \n",
    "        self.caption_length = caption_length\n",
    "        \n",
    "    def _predict_next_words(self, image, partial_caption, n_words=3):\n",
    "        image_features = self.image_encoder.predict(np.expand_dims(image, axis=0))\n",
    "        prediction = self.caption_model.predict([image_features, np.expand_dims(partial_caption, axis=0)])\n",
    "        \n",
    "        next_words = np.argsort(prediction[0])[-n_words:]\n",
    "        next_words_scores = np.sort(prediction[0])[-n_words:]\n",
    "        \n",
    "        return next_words, next_words_scores\n",
    "    \n",
    "    def _decode_caption(self, caption):\n",
    "        return ' '.join([self.inversed_vocabulary[x] for x in caption])\n",
    "    \n",
    "    def generate_captions(self, image, max_caption_length=7, beam_size=3):\n",
    "        partial_captions = [[[self.vocabulary['<s>']], 0]]\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            candidate_partial_captions = []\n",
    "            for partial_caption, score in partial_captions:\n",
    "                if partial_caption[-1] == self.vocabulary['<e>']:\n",
    "                    continue\n",
    "                    \n",
    "                padded_partial_caption = pad_sequences(\n",
    "                    [partial_caption],\n",
    "                    maxlen=self.caption_length, padding='post',\n",
    "                    value=self.vocabulary['<p>']\n",
    "                )[0]\n",
    "                next_words, next_words_scores = self._predict_next_words(image, padded_partial_caption, n_words=beam_size)\n",
    "                \n",
    "                for next_word, next_word_score in zip(next_words, next_words_scores):\n",
    "                    candidate_partial_captions.append([partial_caption + [next_word],\n",
    "                                                       score + next_word_score])\n",
    "                    \n",
    "            partial_captions = sorted(candidate_partial_captions, key=lambda x: x[1] / len(x[0]))[-beam_size:]\n",
    "\n",
    "            done = True\n",
    "            for partial_caption in partial_captions:\n",
    "                if partial_caption[0][-1] != self.vocabulary['<e>'] and len(partial_caption[0]) <= max_caption_length:\n",
    "                    done = False\n",
    "                    break\n",
    "            \n",
    "        decoded_captions = list(map(self._decode_caption, [x[0] for x in partial_captions]))\n",
    "        return decoded_captions[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_encoder = build_image_encoder(dm_val.image_height, dm_train.image_width, dm_train.n_channels)\n",
    "caption_generator = CaptionGenerator(model_path, image_encoder, dm_train.vocabulary, dm_train.caption_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(val_generator)\n",
    "\n",
    "images = batch[0][0]\n",
    "for image in images[:N_SAMPLES]:\n",
    "    captions = caption_generator.generate_captions(image, max_caption_length=MAX_CAPTION_LENGTH, beam_size=BEAM_SIZE)\n",
    "    print('\\n'.join(captions))\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
